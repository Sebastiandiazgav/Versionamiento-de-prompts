{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e533131",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# -- Crear los prompts (v1, v2, v3) --\n",
    "prompt_v1_content = \"\"\"\n",
    "Human: Eres un asistente de viajes. Por favor, genera un itinerario de 3 d√≠as para un viaje a {ciudad}. El viajero est√° interesado en la {interes}.\n",
    "\n",
    "Assistant:\n",
    "\"\"\"\n",
    "with open(\"prompt_template_v1.txt\", \"w\") as f:\n",
    "    f.write(prompt_v1_content)\n",
    "\n",
    "prompt_v2_content = \"\"\"\n",
    "Human: Act√∫a como un experto gu√≠a tur√≠stico. Tu tarea es dise√±ar un itinerario detallado y enriquecedor de 3 d√≠as en {ciudad}. El enfoque principal del viaje debe ser {interes}. Por favor, para cada d√≠a, incluye al menos una sugerencia de restaurante local y un consejo pr√°ctico de transporte.\n",
    "\n",
    "Assistant:\n",
    "\"\"\"\n",
    "with open(\"prompt_template_v2.txt\", \"w\") as f:\n",
    "    f.write(prompt_v2_content)\n",
    "\n",
    "prompt_v3_content = \"\"\"\n",
    "Human: Tu rol es ser un planificador de viajes eficiente para un blog. Genera un itinerario de 3 d√≠as para {ciudad} enfocado en {interes}. La salida debe ser √∫nicamente una tabla en formato Markdown con las columnas: \"D√≠a\", \"Actividad Principal (Enfoque en {interes})\", y \"Sugerencia de Comida\".\n",
    "\n",
    "Assistant:\n",
    "\"\"\"\n",
    "with open(\"prompt_template_v3.txt\", \"w\") as f:\n",
    "    f.write(prompt_v3_content)\n",
    "\n",
    "# -- Crear el archivo Python con la l√≥gica del modelo de Bedrock --\n",
    "bedrock_model_code = \"\"\"\n",
    "import mlflow\n",
    "import mlflow.pyfunc\n",
    "import boto3\n",
    "import json\n",
    "import os\n",
    "\n",
    "class BedrockPromptModel(mlflow.pyfunc.PythonModel):\n",
    "\n",
    "    def load_context(self, context):\n",
    "        self.bedrock_runtime = None\n",
    "        prompt_path = context.artifacts[\"prompt_template\"]\n",
    "        with open(prompt_path, \"r\") as f:\n",
    "            self.prompt_template = f.read()\n",
    "\n",
    "    def _initialize_client(self):\n",
    "        if self.bedrock_runtime is None:\n",
    "            # Apuntando a la regi√≥n de Ohio\n",
    "            self.bedrock_runtime = boto3.client(\n",
    "                service_name=\"bedrock-runtime\",\n",
    "                region_name=\"us-east-2\"\n",
    "            )\n",
    "\n",
    "    def _generate_response(self, prompt):\n",
    "        self._initialize_client()\n",
    "        \n",
    "        provisioned_model_arn = \"arn:aws:bedrock:us-east-2:718199867798:inference-profile/us.anthropic.claude-3-5-sonnet-20241022-v2:0\" \n",
    "        \n",
    "        # El formato del cuerpo para los modelos Claude 3\n",
    "        body = json.dumps({\n",
    "            \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "            \"max_tokens\": 2048,\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [{\"type\": \"text\", \"text\": prompt}]\n",
    "                }\n",
    "            ]\n",
    "        })\n",
    "\n",
    "        response = self.bedrock_runtime.invoke_model(\n",
    "            body=body, \n",
    "            modelId=provisioned_model_arn, # Usamos el ARN aqu√≠\n",
    "            accept=\"application/json\", \n",
    "            contentType=\"application/json\"\n",
    "        )\n",
    "        \n",
    "        response_body = json.loads(response.get(\"body\").read())\n",
    "        return response_body.get(\"content\")[0].get(\"text\")\n",
    "\n",
    "    def predict(self, context, model_input):\n",
    "        outputs = []\n",
    "        for index, row in model_input.iterrows():\n",
    "            prompt_formateado = self.prompt_template.format(\n",
    "                ciudad=row[\"ciudad\"], interes=row[\"interes\"]\n",
    "            )\n",
    "            respuesta = self._generate_response(prompt_formateado)\n",
    "            outputs.append(respuesta)\n",
    "        return outputs\n",
    "\"\"\"\n",
    "with open(\"bedrock_prompt_model.py\", \"w\") as f:\n",
    "    f.write(bedrock_model_code)\n",
    "\n",
    "print(\"‚úÖ Todos los archivos necesarios han sido creados usando la l√≥gica para un modelo aprovisionado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bed01c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from bedrock_prompt_model import BedrockPromptModel\n",
    "\n",
    "# --- Configuraci√≥n ---\n",
    "tracking_server_arn = \"arn:aws:sagemaker:us-east-2:718199867798:mlflow-tracking-server/PruebaMlflow\"\n",
    "mlflow.set_tracking_uri(tracking_server_arn)\n",
    "\n",
    "experiment_name = \"Experimentos_con_Prompts_de_Viajes_SageMaker\"\n",
    "model_name = \"Generador-Itinerarios-Bedrock\" \n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "def register_prompt_version(prompt_file_path, version_tag):\n",
    "    with mlflow.start_run() as run:\n",
    "        print(f\"Registrando versi√≥n con el prompt: {prompt_file_path}\")\n",
    "        artifacts = {\"prompt_template\": prompt_file_path}\n",
    "        mlflow.pyfunc.log_model(\n",
    "            artifact_path=\"model\",\n",
    "            python_model=BedrockPromptModel(),\n",
    "            artifacts=artifacts,\n",
    "            registered_model_name=model_name,\n",
    "        )\n",
    "        mlflow.set_tag(\"prompt_version\", version_tag)\n",
    "        mlflow.set_tag(\"bedrock_model\", \"claude-3.5-sonnet-provisioned\")\n",
    "    print(f\"‚úÖ Versi√≥n '{version_tag}' registrada exitosamente!\")\n",
    "\n",
    "# --- Registrar todas las versiones ---\n",
    "register_prompt_version(\"prompt_template_v1.txt\", \"v1.0 - inicial\")\n",
    "register_prompt_version(\"prompt_template_v2.txt\", \"v2.0 - detallado\")\n",
    "register_prompt_version(\"prompt_template_v3.txt\", \"v3.0 - tabla markdown\")\n",
    "\n",
    "print(\"\\nüéâ Todas las versiones del modelo han sido registradas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ae99ee",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "tracking_server_arn = \"arn:aws:sagemaker:us-east-2:718199867798:mlflow-tracking-server/PruebaMlflow\"\n",
    "mlflow.set_tracking_uri(tracking_server_arn)\n",
    "\n",
    "model_name = \"Generador-Itinerarios-Bedrock\"\n",
    "\n",
    "# Se crea un nuevo experimento para guardar los resultados de las pruebas\n",
    "mlflow.set_experiment(\"Resultados_de_Inferencia\")\n",
    "\n",
    "# Se inicia un \"run\"\n",
    "with mlflow.start_run(run_name=\"Prueba_Manizales_Cafe\"):\n",
    "    \n",
    "    # --- Datos de Entrada ---\n",
    "    input_data_dict = {\n",
    "        \"ciudad\": [\"Manizales\"],\n",
    "        \"interes\": [\"caf√© y paisajes de monta√±a\"]\n",
    "    }\n",
    "    input_data = pd.DataFrame(input_data_dict)\n",
    "    \n",
    "    # guarda la entrada como un artefacto JSON\n",
    "    mlflow.log_dict(input_data_dict, \"input_data.json\")\n",
    "\n",
    "    # --- Cargar y probar la Versi√≥n 2 ---\n",
    "    print(\"Cargando el modelo Versi√≥n 2...\")\n",
    "    model_v2_uri = f\"models:/{model_name}/2\"\n",
    "    loaded_model_v2 = mlflow.pyfunc.load_model(model_v2_uri)\n",
    "    result_v2 = loaded_model_v2.predict(input_data)\n",
    "    \n",
    "    # Se guarda la salida de la v2\n",
    "    mlflow.log_text(result_v2[0], \"output_v2.txt\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\" V2 | Respuesta de la Versi√≥n 2 (Prompt detallado):\")\n",
    "    print(\"=\"*60)\n",
    "    print(result_v2[0])\n",
    "\n",
    "    # --- Cargar y probar la Versi√≥n 3 ---\n",
    "    print(\"\\nCargando el modelo Versi√≥n 3...\")\n",
    "    model_v3_uri = f\"models:/{model_name}/3\" # ‚ö†Ô∏è Ajusta el n√∫mero si es necesario\n",
    "    loaded_model_v3 = mlflow.pyfunc.load_model(model_v3_uri)\n",
    "    result_v3 = loaded_model_v3.predict(input_data)\n",
    "    \n",
    "    # 5. Se guarda la salida de la v3 como un artefacto de texto\n",
    "    mlflow.log_text(result_v3[0], \"output_v3.txt\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\" V3 | Respuesta de la Versi√≥n 3 (Prompt de tabla):\")\n",
    "    print(\"=\"*60)\n",
    "    print(result_v3[0])\n",
    "\n",
    "print(\"\\n‚úÖ Ejecuci√≥n finalizada. Revisa el experimento 'Resultados_de_Inferencia' en MLflow.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
